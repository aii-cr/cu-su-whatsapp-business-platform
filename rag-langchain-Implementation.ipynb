{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80a93a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95862a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"   # adjust as desired\n",
    "COLLECTION_NAME = \"adn\"\n",
    "TOP_K = 6\n",
    "FETCH_K = 24\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976e3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_core.documents import Document\n",
    "import csv\n",
    "from typing import List\n",
    "\n",
    "def load_csv_with_csvloader(csv_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a CSV into Documents using LangChain's CSVLoader.\n",
    "    page_content := only ['title', 'details'].\n",
    "    All remaining columns preserved as metadata (e.g., id, section, tags, channel_number).\n",
    "    \"\"\"\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        cols = csv.DictReader(f).fieldnames or []\n",
    "\n",
    "    content_cols = [c for c in [\"title\", \"details\"] if c in cols]\n",
    "    meta_cols = [c for c in cols if c not in content_cols]\n",
    "    print(f\"content_cols: {content_cols}\")\n",
    "    print(f\"meta_cols: {meta_cols}\")\n",
    "    loader = CSVLoader(\n",
    "        file_path=csv_path,\n",
    "        content_columns=content_cols,\n",
    "        metadata_columns=meta_cols,\n",
    "        encoding=\"utf-8-sig\",\n",
    "        autodetect_encoding=True,\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "# current metadata columns: id,section,subsection,title,details,price_crc,price_text,tags,url,contact_value,channel_number,locale,version\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./RAG_data/adn_rag_base_full_v1_3.csv\",\n",
    "    metadata_columns=[\n",
    "      \"id\",\n",
    "      \"section\",\n",
    "      \"subsection\",\n",
    "      \"title\",\n",
    "      \"details\",\n",
    "      \"price_crc\",\n",
    "      \"price_text\",\n",
    "      \"tags\",\n",
    "      \"url\",\n",
    "      \"contact_value\",\n",
    "      \"channel_number\",\n",
    "      \"locale\",\n",
    "      \"version\"\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4ad441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_cols: ['title', 'details']\n",
      "meta_cols: ['id', 'section', 'subsection', 'price_crc', 'price_text', 'tags', 'url', 'contact_value', 'channel_number', 'locale', 'version']\n",
      "title: Descripci√≥n\n",
      "details: American Data Networks S.A. (ADN) fundada en 2005. Equipo con m√°s de 15 a√±os de experiencia en telecomunicaciones.\n",
      "Enfoque 100 % en calidad de servicio y soporte oportuno. Actualizaci√≥n constante de tecnolog√≠as para brindar servicios de clase mundial.\n",
      "Opci√≥n de transporte nacional e internacional de alta capacidad de datos en Costa Rica; permite optimizar y expandir redes de forma segura y r√°pida.\n",
      "{'source': 'RAG_data/adn_rag_base_full_v1_3.csv', 'row': 0, 'id': '2eebd6ef-cd3e-46e4-a268-dd4830bf76aa', 'section': 'Compa√±√≠a', 'subsection': 'Qui√©nes Somos', 'price_crc': '', 'price_text': '', 'tags': 'empresa, historia, calidad, telecomunicaciones', 'url': '', 'contact_value': '', 'channel_number': '', 'locale': 'es_CR', 'version': 'v1.3'}\n"
     ]
    }
   ],
   "source": [
    "adn_data = load_csv_with_csvloader(\"RAG_data/adn_rag_base_full_v1_3.csv\")\n",
    "\n",
    "for doc in adn_data:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf01a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'RAG_data/adn_rag_base_full_v1_3.csv', 'row': 0, 'id': '2eebd6ef-cd3e-46e4-a268-dd4830bf76aa', 'section': 'Compa√±√≠a', 'subsection': 'Qui√©nes Somos', 'price_crc': '', 'price_text': '', 'tags': 'empresa, historia, calidad, telecomunicaciones', 'url': '', 'contact_value': '', 'channel_number': '', 'locale': 'es_CR', 'version': 'v1.3'}, page_content='title: Descripci√≥n\\ndetails: American Data Networks S.A. (ADN) fundada en 2005. Equipo con m√°s de 15 a√±os de experiencia en telecomunicaciones.\\nEnfoque 100 % en calidad de servicio y soporte oportuno. Actualizaci√≥n constante de tecnolog√≠as para brindar servicios de clase mundial.\\nOpci√≥n de transporte nacional e internacional de alta capacidad de datos en Costa Rica; permite optimizar y expandir redes de forma segura y r√°pida.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adn_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06587a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    adn_data,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbddc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "\n",
    "semantic_documents = semantic_chunker.split_documents(adn_data)\n",
    "\n",
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"ADN_Data_Semantic_Chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe044cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 6})\n",
    "\n",
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 6})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782a5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "Eres un asistente de soporte que trabaja para American Data Networks. Habla siempre en primera persona como si fueras parte del equipo. Responde SOLO con la informaci√≥n del CONTEXTO proporcionado.\n",
    "Si la pregunta no se encuentra en el contexto proporcionado de American Data Networks, responde: \"No tengo la respuesta a esa pregunta\".\n",
    "Si la pregunta es sobre un producto o servicio que no es de American Data Networks, responde: \"No tenemos informaci√≥n sobre ese producto o servicio\".\n",
    "\n",
    "Responde en el mismo idioma en que se hizo la pregunta.\n",
    "Importante: usa expresiones en primera persona (por ejemplo: ‚Äúnuestra direcci√≥n‚Äù, ‚Äúpuedes visitarnos‚Äù, ‚Äúpodemos ayudarte‚Äù), nunca en tercera persona (‚Äúellos‚Äù, ‚Äútienen‚Äù, ‚Äúpuedes visitarlos‚Äù).\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bfc073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31eca756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb1110c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c48bc68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S√≠, en nuestra grilla IPTV tengo disponibles los canales History Channel y History Channel 2.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"tienen History Channel?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73de6d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para contratar un servicio de ADN, puedes hacerlo en l√≠nea visitando https://data.cr/adquirir-servicios. Tambi√©n puedes contratar por WhatsApp, donde un asesor verificar√° la cobertura, te recomendar√° el plan, registrar√° tus datos y compartir√° el enlace de pago o agenda. Si deseas, puedo ayudarte a comenzar con el proceso o proporcionarte m√°s detalles.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Quiero contratar un servicio de ADN\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c9b254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El plan residencial de 100/100 Mbps tiene un precio de ‚Ç°20,500 IVI. Puedes contactarnos si necesitas m√°s informaci√≥n o si deseas contratar este plan.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(adn_data,)\n",
    "\n",
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "bm25_retrieval_chain.invoke({\"question\" : \"Precio de un plan de 100/100 Mbps\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f7e469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nuestro plan de 100 megas es el Plan 100/100 Mbps, que es un plan residencial sim√©trico e incluye Equipo Wi-Fi, Firewall y Soporte 24/7, y su precio es de ‚Ç°20,500 IVI. \\n\\nAdem√°s, tenemos otros planes populares como el Plan 500/500 Mbps por ‚Ç°33,410 IVI, y el Plan 1/1 Gbps por ‚Ç°26,600 IVI. Los planes m√°s populares son el de 500/500 Mbps y el de 1 Gbps. \\n\\nTambi√©n puedes contratar en l√≠nea, seleccionando el plan y los adicionales que desees, y siguiendo los pasos para completar tu proceso de contrataci√≥n.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")\n",
    "\n",
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "\n",
    "multi_query_retrieval_chain.invoke({\"question\" : \"Precio de un plan de 100 megas, y que mas tienes, y cuales son los planes mas populares\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3bd0bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nuestro plan de 100 megas cuesta ‚Ç°20,500 IVI. Adem√°s, tenemos otros planes como el de 250 megas por ‚Ç°26,600 IVI y el plan m√°s popular de 500 megas por ‚Ç°33,410 IVI. Si quieres m√°s informaci√≥n sobre otros planes o detalles espec√≠ficos, puedo ayudarte con eso.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = adn_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)\n",
    "\n",
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "\n",
    "parent_document_retrieval_chain.invoke({\"question\" : \"Precio de un plan de 100 megas, y que mas tienes, y cual es el plan mas popular\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20394720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e793448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test data for ALL retrievers...\n",
      "\n",
      "Generating test data for NAIVE retriever...\n",
      "‚úì Processed question 1: ¬øY la visi√≥n de la empresa cu√°l es? Y de...\n",
      "‚úì Processed question 2: ¬øTienen plan de 1 giga sim√©trico? ¬øEn cu...\n",
      "‚úì Processed question 3: ¬øQu√© incluyen los planes residenciales a...\n",
      "\n",
      "Generating test data for SEMANTIC retriever...\n",
      "‚úì Processed question 1: ¬øY la visi√≥n de la empresa cu√°l es? Y de...\n",
      "‚úì Processed question 2: ¬øTienen plan de 1 giga sim√©trico? ¬øEn cu...\n",
      "‚úì Processed question 3: ¬øQu√© incluyen los planes residenciales a...\n",
      "\n",
      "Generating test data for BM25 retriever...\n",
      "‚úì Processed question 1: ¬øY la visi√≥n de la empresa cu√°l es? Y de...\n",
      "‚úì Processed question 2: ¬øTienen plan de 1 giga sim√©trico? ¬øEn cu...\n",
      "‚úì Processed question 3: ¬øQu√© incluyen los planes residenciales a...\n",
      "\n",
      "Generating test data for MULTI_QUERY retriever...\n",
      "‚úì Processed question 1: ¬øY la visi√≥n de la empresa cu√°l es? Y de...\n",
      "‚úì Processed question 2: ¬øTienen plan de 1 giga sim√©trico? ¬øEn cu...\n",
      "‚úì Processed question 3: ¬øQu√© incluyen los planes residenciales a...\n",
      "\n",
      "Generating test data for PARENT_DOCUMENT retriever...\n",
      "‚úì Processed question 1: ¬øY la visi√≥n de la empresa cu√°l es? Y de...\n",
      "‚úì Processed question 2: ¬øTienen plan de 1 giga sim√©trico? ¬øEn cu...\n",
      "‚úì Processed question 3: ¬øQu√© incluyen los planes residenciales a...\n",
      "\n",
      "‚úì All retriever test data generated!\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from typing import List\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "\n",
    "# Create manual test set with realistic user questions\n",
    "manual_test_set_updated = [\n",
    "    # {\n",
    "    #     \"question\": \"¬øQu√© es American Data Networks (ADN) y desde cu√°ndo existe?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"American Data Networks S.A. (ADN) es una empresa de telecomunicaciones fundada en 2005.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¬øCu√°l es la misi√≥n de ADN?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"Ser la primera opci√≥n en servicios de telecomunicaciones, garantizando transporte de datos r√°pido, eficiente y seguro.\"\n",
    "    # },\n",
    "    {\n",
    "        \"question\": \"¬øY la visi√≥n de la empresa cu√°l es? Y desde cu√°ndo existe?\",\n",
    "        \"contexts\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"ground_truth\": \"Ser la compa√±√≠a l√≠der en vanguardia tecnol√≥gica en Costa Rica, con enfoque claro en clientes y est√°ndares de clase mundial. Fue Fundada en 2005\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"question\": \"Estoy armando mi plan en casa: ¬øcu√°nto cuesta el plan residencial de 100/100 megas?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"‚Ç°20 500 IVI.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¬øCu√°l es el precio del plan de 250/250 megas sim√©tricos?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"‚Ç°26 600 IVI.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quiero m√°s velocidad: ¬øcu√°nto vale el plan de 500/500 megas?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"‚Ç°33 410 IVI.\"\n",
    "    # },\n",
    "    {\n",
    "        \"question\": \"¬øTienen plan de 1 giga sim√©trico? ¬øEn cu√°nto sale al mes?\",\n",
    "        \"contexts\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"ground_truth\": \"‚Ç°49 500 IVI por mes.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"¬øQu√© incluyen los planes residenciales adem√°s del Internet?\",\n",
    "        \"contexts\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"ground_truth\": \"Incluyen equipo Wi-Fi, firewall gestionado y soporte t√©cnico 24/7.\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"question\": \"Quiero agregar telefon√≠a fija a mi plan: ¬øcu√°nto cuesta al mes?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"‚Ç°3 590 IVI/mes.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Si compro IPTV, ¬øcu√°ntos dispositivos como m√°ximo puedo adquirir?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"Hasta 10 dispositivos IPTV.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¬øY cu√°nto cuesta cada dispositivo IPTV?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"‚Ç°4 590 IVI por dispositivo.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"M√°s o menos, ¬øcu√°ntos canales trae el servicio de IPTV?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"M√°s de 100 canales.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¬øQu√© impuestos y cargos ya vienen incluidos en los precios IVI?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"IVA 13 %, 1 % Cruz Roja y 0,75 % 9-1-1.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¬øCu√°l es el n√∫mero de WhatsApp para comprar o pedir soporte?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"+506 7087-8240.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Necesito llamarles: ¬øcu√°l es el tel√©fono principal de ADN?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"+506 4050-5050.\"\n",
    "    # }\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Generating test data for ALL retrievers...\")\n",
    "\n",
    "# Initialize storage for all retrievers\n",
    "retriever_responses = {\n",
    "    \"naive\": [],\n",
    "    \"semantic\": [],\n",
    "    \"bm25\": [],\n",
    "    \"multi_query\": [],\n",
    "    \"parent_document\": []\n",
    "}\n",
    "\n",
    "# Dictionary of retrieval chains\n",
    "retrieval_chains = {\n",
    "    \"naive\": naive_retrieval_chain,\n",
    "    \"semantic\": semantic_retrieval_chain,\n",
    "    \"bm25\": bm25_retrieval_chain,\n",
    "    \"multi_query\": multi_query_retrieval_chain,\n",
    "    \"parent_document\": parent_document_retrieval_chain\n",
    "}\n",
    "\n",
    "# Generate responses for each retriever\n",
    "for retriever_name, chain in retrieval_chains.items():\n",
    "    print(f\"\\nGenerating test data for {retriever_name.upper()} retriever...\")\n",
    "    \n",
    "    for i, item in enumerate(manual_test_set_updated):\n",
    "        question = item[\"question\"]\n",
    "        \n",
    "        try:\n",
    "            # Use the specific retrieval chain\n",
    "            response = chain.invoke({\"question\": question})\n",
    "            \n",
    "            updated_item = {\n",
    "                \"question\": question,\n",
    "                \"contexts\": [doc.page_content for doc in response['context']],\n",
    "                \"answer\": response['response'].content,\n",
    "                \"ground_truth\": item[\"ground_truth\"]\n",
    "            }\n",
    "            \n",
    "            retriever_responses[retriever_name].append(updated_item)\n",
    "            print(f\"‚úì Processed question {i+1}: {question[:40]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing question {i+1} for {retriever_name}: {str(e)}\")\n",
    "            # Add placeholder data to maintain consistency\n",
    "            updated_item = {\n",
    "                \"question\": question,\n",
    "                \"contexts\": [\"Error: Could not retrieve context\"],\n",
    "                \"answer\": \"Error: Could not generate answer\",\n",
    "                \"ground_truth\": item[\"ground_truth\"]\n",
    "            }\n",
    "            retriever_responses[retriever_name].append(updated_item)\n",
    "\n",
    "print(\"\\n‚úì All retriever test data generated!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c38bbe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive dataset shape: (3, 4)\n",
      "Semantic dataset shape: (3, 4)\n",
      "Bm25 dataset shape: (3, 4)\n",
      "Multi_query dataset shape: (3, 4)\n",
      "Parent_document dataset shape: (3, 4)\n",
      "\n",
      "‚úì All datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Create datasets for all retrievers\n",
    "retriever_datasets = {}\n",
    "retriever_dataframes = {}\n",
    "\n",
    "for retriever_name, responses in retriever_responses.items():\n",
    "    df = pd.DataFrame(responses)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    retriever_datasets[retriever_name] = dataset\n",
    "    retriever_dataframes[retriever_name] = df\n",
    "    \n",
    "    print(f\"{retriever_name.capitalize()} dataset shape: {df.shape}\")\n",
    "\n",
    "print(\"\\n‚úì All datasets created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ef78e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation of all retrievers...\n",
      "============================================================\n",
      "\n",
      "Evaluating NAIVE retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c06c2ee13f4368bf600a9628e1c13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Naive evaluation completed successfully!\n",
      "  faithfulness: 0.9333\n",
      "  answer_relevancy: 0.9188\n",
      "  context_precision: 0.3931\n",
      "  context_recall: 0.6667\n",
      "\n",
      "Evaluating SEMANTIC retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35acd67d27a74eddb947d26548a32e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semantic evaluation completed successfully!\n",
      "  faithfulness: 0.9333\n",
      "  answer_relevancy: 0.9165\n",
      "  context_precision: 0.2264\n",
      "  context_recall: 0.6667\n",
      "\n",
      "Evaluating BM25 retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8dd84beb0f46bb9caf7763ad743c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bm25 evaluation completed successfully!\n",
      "  faithfulness: 1.0000\n",
      "  answer_relevancy: 0.3249\n",
      "  context_precision: 0.0000\n",
      "  context_recall: 0.0000\n",
      "\n",
      "Evaluating MULTI_QUERY retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53288e8f274a4f2ebe5289d96a994d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi_query evaluation completed successfully!\n",
      "  faithfulness: 0.8917\n",
      "  answer_relevancy: 0.9239\n",
      "  context_precision: 0.2819\n",
      "  context_recall: 0.6667\n",
      "\n",
      "Evaluating PARENT_DOCUMENT retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aabfe3e692d4289a63b5998c523bacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parent_document evaluation completed successfully!\n",
      "  faithfulness: 0.9333\n",
      "  answer_relevancy: 0.9231\n",
      "  context_precision: 0.4630\n",
      "  context_recall: 0.6667\n",
      "\n",
      "============================================================\n",
      "‚úÖ ALL EVALUATIONS COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy, \n",
    "    context_precision,\n",
    "    context_recall\n",
    "]\n",
    "\n",
    "# Storage for evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "def extract_score_from_result(result, metric_name):\n",
    "    \"\"\"Safely extract score from RAGAS EvaluationResult object.\"\"\"\n",
    "    try:\n",
    "        # Try dictionary-style access first\n",
    "        score = result[metric_name]\n",
    "        if isinstance(score, list):\n",
    "            # Calculate average for list of scores\n",
    "            valid_scores = [s for s in score if s is not None and not np.isnan(s)]\n",
    "            return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
    "        elif isinstance(score, (int, float)) and not np.isnan(score):\n",
    "            return float(score)\n",
    "        else:\n",
    "            return 0.0\n",
    "    except (KeyError, TypeError, AttributeError):\n",
    "        try:\n",
    "            # Try attribute access as fallback\n",
    "            score = getattr(result, metric_name, None)\n",
    "            if score is not None:\n",
    "                if isinstance(score, list):\n",
    "                    valid_scores = [s for s in score if s is not None and not np.isnan(s)]\n",
    "                    return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
    "                elif isinstance(score, (int, float)) and not np.isnan(score):\n",
    "                    return float(score)\n",
    "            return 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "print(\"Starting comprehensive evaluation of all retrievers...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for retriever_name, dataset in retriever_datasets.items():\n",
    "    print(f\"\\nEvaluating {retriever_name.upper()} retriever...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = evaluate(\n",
    "            dataset,\n",
    "            metrics=metrics,\n",
    "            llm=generator_llm,\n",
    "            embeddings=generator_embeddings\n",
    "        )\n",
    "        \n",
    "        evaluation_results[retriever_name] = result\n",
    "        print(f\"‚úÖ {retriever_name.capitalize()} evaluation completed successfully!\")\n",
    "        \n",
    "        # Show immediate results using corrected extraction\n",
    "        for metric_name in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:\n",
    "            avg_score = extract_score_from_result(result, metric_name)\n",
    "            print(f\"  {metric_name}: {avg_score:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error evaluating {retriever_name}: {str(e)}\")\n",
    "        # Create placeholder result\n",
    "        evaluation_results[retriever_name] = {\n",
    "            'faithfulness': 0.0,\n",
    "            'answer_relevancy': 0.0, \n",
    "            'context_precision': 0.0,\n",
    "            'context_recall': 0.0\n",
    "        }\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ALL EVALUATIONS COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33eb1c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "                    COMPREHENSIVE RAG RETRIEVER PERFORMANCE COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "           Metric  Naive  Semantic   Bm25  Multi Query  Parent Document                      Description\n",
      "     Faithfulness 0.9333    0.9333 1.0000       0.8917           0.9333 Factual consistency with context\n",
      " Answer Relevancy 0.9188    0.9165 0.3249       0.9239           0.9231        Relevance to the question\n",
      "Context Precision 0.3931    0.2264 0.0000       0.2819           0.4630   Precision of retrieved context\n",
      "   Context Recall 0.6667    0.6667 0.0000       0.6667           0.6667       Recall of relevant context\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                                          OVERALL AVERAGES                                          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "ü•á Parent Document     : 0.7465\n",
      "ü•à Naive               : 0.7280\n",
      "ü•â Multi Query         : 0.6910\n",
      "4Ô∏è‚É£ Semantic            : 0.6857\n",
      "5Ô∏è‚É£ Bm25                : 0.6624\n",
      "\n",
      "üèÜ WINNER: PARENT DOCUMENT RETRIEVER!\n",
      "Best Overall Score: 0.7465\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_score_safe(result, metric_name):\n",
    "    \"\"\"Safely extract score from RAGAS result with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        # Handle both EvaluationResult objects and dictionaries\n",
    "        if hasattr(result, '__getitem__'):\n",
    "            try:\n",
    "                score = result[metric_name]\n",
    "            except (KeyError, TypeError):\n",
    "                # Fallback to attribute access\n",
    "                score = getattr(result, metric_name, None)\n",
    "        else:\n",
    "            # Try attribute access first\n",
    "            score = getattr(result, metric_name, None)\n",
    "        \n",
    "        if score is None:\n",
    "            return 0.0\n",
    "            \n",
    "        if isinstance(score, list):\n",
    "            # Calculate average for list of scores\n",
    "            valid_scores = [s for s in score if s is not None and not (isinstance(s, float) and np.isnan(s))]\n",
    "            return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
    "        elif isinstance(score, (int, float)) and not np.isnan(score):\n",
    "            return float(score)\n",
    "        else:\n",
    "            return 0.0\n",
    "    except (KeyError, TypeError, AttributeError) as e:\n",
    "        print(f\"Warning: Could not extract {metric_name}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# Extract scores for all retrievers\n",
    "metrics_names = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "retriever_names = ['naive', 'semantic', 'bm25', 'multi_query', 'parent_document']\n",
    "\n",
    "# Create comprehensive results matrix\n",
    "results_matrix = {}\n",
    "for retriever_name in retriever_names:\n",
    "    result = evaluation_results[retriever_name]\n",
    "    scores = [extract_score_safe(result, metric) for metric in metrics_names]\n",
    "    results_matrix[retriever_name] = scores\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Faithfulness',\n",
    "        'Answer Relevancy',\n",
    "        'Context Precision', \n",
    "        'Context Recall'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add columns for each retriever\n",
    "for retriever_name in retriever_names:\n",
    "    comparison_data[retriever_name.replace('_', ' ').title()] = results_matrix[retriever_name]\n",
    "\n",
    "# Add descriptions\n",
    "comparison_data['Description'] = [\n",
    "    'Factual consistency with context',\n",
    "    'Relevance to the question',\n",
    "    'Precision of retrieved context',\n",
    "    'Recall of relevant context'\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Calculate averages for each retriever\n",
    "averages = {}\n",
    "for retriever_name in retriever_names:\n",
    "    scores = results_matrix[retriever_name]\n",
    "    valid_scores = [score for score in scores if score > 0]\n",
    "    averages[retriever_name] = np.mean(valid_scores) if valid_scores else 0.0\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"                    COMPREHENSIVE RAG RETRIEVER PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "# Display main comparison table\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "print()\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'OVERALL AVERAGES':^100}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Sort retrievers by performance\n",
    "sorted_retrievers = sorted(averages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (retriever_name, avg_score) in enumerate(sorted_retrievers):\n",
    "    rank_emoji = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else f\"{i+1}Ô∏è‚É£\"\n",
    "    print(f\"{rank_emoji} {retriever_name.replace('_', ' ').title():<20}: {avg_score:.4f}\")\n",
    "\n",
    "print()\n",
    "best_retriever = sorted_retrievers[0][0]\n",
    "best_score = sorted_retrievers[0][1]\n",
    "print(f\"üèÜ WINNER: {best_retriever.replace('_', ' ').title().upper()} RETRIEVER!\")\n",
    "print(f\"Best Overall Score: {best_score:.4f}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77847653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                        DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "METRIC-BY-METRIC WINNERS:\n",
      "----------------------------------------\n",
      "üìä Faithfulness:\n",
      "   üèÜ Best:  Bm25 (1.0000)\n",
      "   üìâ Worst: Multi Query (0.8917)\n",
      "\n",
      "üìä Answer Relevancy:\n",
      "   üèÜ Best:  Multi Query (0.9239)\n",
      "   üìâ Worst: Bm25 (0.3249)\n",
      "\n",
      "üìä Context Precision:\n",
      "   üèÜ Best:  Parent Document (0.4630)\n",
      "   üìâ Worst: Bm25 (0.0000)\n",
      "\n",
      "üìä Context Recall:\n",
      "   üèÜ Best:  Naive (0.6667)\n",
      "   üìâ Worst: Bm25 (0.0000)\n",
      "\n",
      "RETRIEVER STRENGTHS & WEAKNESSES:\n",
      "---------------------------------------------\n",
      "üîç Naive:\n",
      "   ‚úÖ Strongest: Faithfulness (0.9333)\n",
      "   ‚ö†Ô∏è  Weakest:  Context Precision (0.3931)\n",
      "\n",
      "üîç Semantic:\n",
      "   ‚úÖ Strongest: Faithfulness (0.9333)\n",
      "   ‚ö†Ô∏è  Weakest:  Context Precision (0.2264)\n",
      "\n",
      "üîç Bm25:\n",
      "   ‚úÖ Strongest: Faithfulness (1.0000)\n",
      "   ‚ö†Ô∏è  Weakest:  Context Precision (0.0000)\n",
      "\n",
      "üîç Multi Query:\n",
      "   ‚úÖ Strongest: Answer Relevancy (0.9239)\n",
      "   ‚ö†Ô∏è  Weakest:  Context Precision (0.2819)\n",
      "\n",
      "üîç Parent Document:\n",
      "   ‚úÖ Strongest: Faithfulness (0.9333)\n",
      "   ‚ö†Ô∏è  Weakest:  Context Precision (0.4630)\n",
      "\n",
      "üéØ PRODUCTION RECOMMENDATIONS:\n",
      "-----------------------------------\n",
      "‚úÖ Deploy PARENT DOCUMENT RETRIEVER for production\n",
      "‚úÖ Excellent for maintaining document context integrity\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                        DETAILED PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Metric-by-metric analysis\n",
    "print(\"\\nMETRIC-BY-METRIC WINNERS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, metric in enumerate(comparison_df['Metric']):\n",
    "    metric_scores = {}\n",
    "    for j, retriever_name in enumerate(retriever_names):\n",
    "        col_name = retriever_name.replace('_', ' ').title()\n",
    "        metric_scores[retriever_name] = comparison_df.iloc[i][col_name]\n",
    "    \n",
    "    # Find best performer for this metric\n",
    "    best_performer = max(metric_scores.items(), key=lambda x: x[1])\n",
    "    worst_performer = min(metric_scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"üìä {metric}:\")\n",
    "    print(f\"   üèÜ Best:  {best_performer[0].replace('_', ' ').title()} ({best_performer[1]:.4f})\")\n",
    "    print(f\"   üìâ Worst: {worst_performer[0].replace('_', ' ').title()} ({worst_performer[1]:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Strengths and weaknesses analysis\n",
    "print(\"RETRIEVER STRENGTHS & WEAKNESSES:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for retriever_name in retriever_names:\n",
    "    scores = results_matrix[retriever_name]\n",
    "    best_metric_idx = np.argmax(scores)\n",
    "    worst_metric_idx = np.argmin(scores)\n",
    "    \n",
    "    best_metric = metrics_names[best_metric_idx]\n",
    "    worst_metric = metrics_names[worst_metric_idx]\n",
    "    \n",
    "    print(f\"üîç {retriever_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"   ‚úÖ Strongest: {best_metric.replace('_', ' ').title()} ({scores[best_metric_idx]:.4f})\")\n",
    "    print(f\"   ‚ö†Ô∏è  Weakest:  {worst_metric.replace('_', ' ').title()} ({scores[worst_metric_idx]:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Recommendations\n",
    "print(\"üéØ PRODUCTION RECOMMENDATIONS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "if best_retriever == \"semantic\":\n",
    "    print(\"‚úÖ Deploy SEMANTIC RETRIEVER for production\")\n",
    "    print(\"‚úÖ Excellent balance of context precision and recall\")\n",
    "elif best_retriever == \"multi_query\":\n",
    "    print(\"‚úÖ Deploy MULTI-QUERY RETRIEVER for production\") \n",
    "    print(\"‚úÖ Superior query understanding and context retrieval\")\n",
    "elif best_retriever == \"parent_document\":\n",
    "    print(\"‚úÖ Deploy PARENT DOCUMENT RETRIEVER for production\")\n",
    "    print(\"‚úÖ Excellent for maintaining document context integrity\")\n",
    "elif best_retriever == \"bm25\":\n",
    "    print(\"‚úÖ Deploy BM25 RETRIEVER for production\")\n",
    "    print(\"‚úÖ Strong traditional IR performance with keyword matching\")\n",
    "else:\n",
    "    print(\"‚úÖ Deploy NAIVE RETRIEVER for production\")\n",
    "    print(\"‚úÖ Simple and effective baseline performance\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
