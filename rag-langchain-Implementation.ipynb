{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80a93a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95862a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"   # adjust as desired\n",
    "COLLECTION_NAME = \"adn\"\n",
    "TOP_K = 6\n",
    "FETCH_K = 24\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976e3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_core.documents import Document\n",
    "import csv\n",
    "from typing import List\n",
    "\n",
    "def load_csv_with_csvloader(csv_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a CSV into Documents using LangChain's CSVLoader.\n",
    "    page_content := only ['title', 'details'].\n",
    "    All remaining columns preserved as metadata (e.g., id, section, tags, channel_number).\n",
    "    \"\"\"\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        cols = csv.DictReader(f).fieldnames or []\n",
    "\n",
    "    content_cols = [c for c in [\"title\", \"details\"] if c in cols]\n",
    "    meta_cols = [c for c in cols if c not in content_cols]\n",
    "    print(f\"content_cols: {content_cols}\")\n",
    "    print(f\"meta_cols: {meta_cols}\")\n",
    "    loader = CSVLoader(\n",
    "        file_path=csv_path,\n",
    "        content_columns=content_cols,\n",
    "        metadata_columns=meta_cols,\n",
    "        encoding=\"utf-8-sig\",\n",
    "        autodetect_encoding=True,\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "# current metadata columns: id,section,subsection,title,details,price_crc,price_text,tags,url,contact_value,channel_number,locale,version\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./RAG_data/adn_rag_base_full_v1_3.csv\",\n",
    "    metadata_columns=[\n",
    "      \"id\",\n",
    "      \"section\",\n",
    "      \"subsection\",\n",
    "      \"title\",\n",
    "      \"details\",\n",
    "      \"price_crc\",\n",
    "      \"price_text\",\n",
    "      \"tags\",\n",
    "      \"url\",\n",
    "      \"contact_value\",\n",
    "      \"channel_number\",\n",
    "      \"locale\",\n",
    "      \"version\"\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4ad441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_cols: ['title', 'details']\n",
      "meta_cols: ['id', 'section', 'subsection', 'price_crc', 'price_text', 'tags', 'url', 'contact_value', 'channel_number', 'locale', 'version']\n",
      "title: Descripción\n",
      "details: American Data Networks S.A. (ADN) fundada en 2005. Equipo con más de 15 años de experiencia en telecomunicaciones.\n",
      "Enfoque 100 % en calidad de servicio y soporte oportuno. Actualización constante de tecnologías para brindar servicios de clase mundial.\n",
      "Opción de transporte nacional e internacional de alta capacidad de datos en Costa Rica; permite optimizar y expandir redes de forma segura y rápida.\n",
      "{'source': 'RAG_data/adn_rag_base_full_v1_3.csv', 'row': 0, 'id': '2eebd6ef-cd3e-46e4-a268-dd4830bf76aa', 'section': 'Compañía', 'subsection': 'Quiénes Somos', 'price_crc': '', 'price_text': '', 'tags': 'empresa, historia, calidad, telecomunicaciones', 'url': '', 'contact_value': '', 'channel_number': '', 'locale': 'es_CR', 'version': 'v1.3'}\n"
     ]
    }
   ],
   "source": [
    "adn_data = load_csv_with_csvloader(\"RAG_data/adn_rag_base_full_v1_3.csv\")\n",
    "\n",
    "for doc in adn_data:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf01a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'RAG_data/adn_rag_base_full_v1_3.csv', 'row': 0, 'id': '2eebd6ef-cd3e-46e4-a268-dd4830bf76aa', 'section': 'Compañía', 'subsection': 'Quiénes Somos', 'price_crc': '', 'price_text': '', 'tags': 'empresa, historia, calidad, telecomunicaciones', 'url': '', 'contact_value': '', 'channel_number': '', 'locale': 'es_CR', 'version': 'v1.3'}, page_content='title: Descripción\\ndetails: American Data Networks S.A. (ADN) fundada en 2005. Equipo con más de 15 años de experiencia en telecomunicaciones.\\nEnfoque 100 % en calidad de servicio y soporte oportuno. Actualización constante de tecnologías para brindar servicios de clase mundial.\\nOpción de transporte nacional e internacional de alta capacidad de datos en Costa Rica; permite optimizar y expandir redes de forma segura y rápida.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adn_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06587a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    adn_data,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbddc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "\n",
    "semantic_documents = semantic_chunker.split_documents(adn_data)\n",
    "\n",
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"ADN_Data_Semantic_Chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe044cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 6})\n",
    "\n",
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 6})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782a5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "Eres un asistente de soporte que trabaja para American Data Networks. Habla siempre en primera persona como si fueras parte del equipo. Responde SOLO con la información del CONTEXTO proporcionado.\n",
    "Si la pregunta no se encuentra en el contexto proporcionado de American Data Networks, responde: \"No tengo la respuesta a esa pregunta\".\n",
    "Si la pregunta es sobre un producto o servicio que no es de American Data Networks, responde: \"No tenemos información sobre ese producto o servicio\".\n",
    "\n",
    "Responde en el mismo idioma en que se hizo la pregunta.\n",
    "Importante: usa expresiones en primera persona (por ejemplo: “nuestra dirección”, “puedes visitarnos”, “podemos ayudarte”), nunca en tercera persona (“ellos”, “tienen”, “puedes visitarlos”).\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bfc073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31eca756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb1110c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c48bc68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sí, en nuestra grilla IPTV tengo disponibles los canales History Channel y History Channel 2.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"tienen History Channel?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73de6d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para contratar un servicio de ADN, puedes hacerlo en línea visitando https://data.cr/adquirir-servicios. También puedes contratar por WhatsApp, donde un asesor verificará la cobertura, te recomendará el plan, registrará tus datos y compartirá el enlace de pago o agenda. Si deseas, puedo ayudarte a comenzar con el proceso o proporcionarte más detalles.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Quiero contratar un servicio de ADN\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c9b254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El plan residencial de 100/100 Mbps tiene un precio de ₡20,500 IVI. Puedes contactarnos si necesitas más información o si deseas contratar este plan.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(adn_data,)\n",
    "\n",
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "bm25_retrieval_chain.invoke({\"question\" : \"Precio de un plan de 100/100 Mbps\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f7e469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nuestro plan de 100 megas es el Plan 100/100 Mbps, que es un plan residencial simétrico e incluye Equipo Wi-Fi, Firewall y Soporte 24/7, y su precio es de ₡20,500 IVI. \\n\\nAdemás, tenemos otros planes populares como el Plan 500/500 Mbps por ₡33,410 IVI, y el Plan 1/1 Gbps por ₡26,600 IVI. Los planes más populares son el de 500/500 Mbps y el de 1 Gbps. \\n\\nTambién puedes contratar en línea, seleccionando el plan y los adicionales que desees, y siguiendo los pasos para completar tu proceso de contratación.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")\n",
    "\n",
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "\n",
    "multi_query_retrieval_chain.invoke({\"question\" : \"Precio de un plan de 100 megas, y que mas tienes, y cuales son los planes mas populares\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3bd0bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nuestro plan de 100 megas cuesta ₡20,500 IVI. Además, tenemos otros planes como el de 250 megas por ₡26,600 IVI y el plan más popular de 500 megas por ₡33,410 IVI. Si quieres más información sobre otros planes o detalles específicos, puedo ayudarte con eso.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = adn_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)\n",
    "\n",
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "\n",
    "parent_document_retrieval_chain.invoke({\"question\" : \"Precio de un plan de 100 megas, y que mas tienes, y cual es el plan mas popular\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20394720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e793448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test data for ALL retrievers...\n",
      "\n",
      "Generating test data for NAIVE retriever...\n",
      "✓ Processed question 1: ¿Y la visión de la empresa cuál es? Y de...\n",
      "✓ Processed question 2: ¿Tienen plan de 1 giga simétrico? ¿En cu...\n",
      "✓ Processed question 3: ¿Qué incluyen los planes residenciales a...\n",
      "\n",
      "Generating test data for SEMANTIC retriever...\n",
      "✓ Processed question 1: ¿Y la visión de la empresa cuál es? Y de...\n",
      "✓ Processed question 2: ¿Tienen plan de 1 giga simétrico? ¿En cu...\n",
      "✓ Processed question 3: ¿Qué incluyen los planes residenciales a...\n",
      "\n",
      "Generating test data for BM25 retriever...\n",
      "✓ Processed question 1: ¿Y la visión de la empresa cuál es? Y de...\n",
      "✓ Processed question 2: ¿Tienen plan de 1 giga simétrico? ¿En cu...\n",
      "✓ Processed question 3: ¿Qué incluyen los planes residenciales a...\n",
      "\n",
      "Generating test data for MULTI_QUERY retriever...\n",
      "✓ Processed question 1: ¿Y la visión de la empresa cuál es? Y de...\n",
      "✓ Processed question 2: ¿Tienen plan de 1 giga simétrico? ¿En cu...\n",
      "✓ Processed question 3: ¿Qué incluyen los planes residenciales a...\n",
      "\n",
      "Generating test data for PARENT_DOCUMENT retriever...\n",
      "✓ Processed question 1: ¿Y la visión de la empresa cuál es? Y de...\n",
      "✓ Processed question 2: ¿Tienen plan de 1 giga simétrico? ¿En cu...\n",
      "✓ Processed question 3: ¿Qué incluyen los planes residenciales a...\n",
      "\n",
      "✓ All retriever test data generated!\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from typing import List\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "\n",
    "# Create manual test set with realistic user questions\n",
    "manual_test_set_updated = [\n",
    "    # {\n",
    "    #     \"question\": \"¿Qué es American Data Networks (ADN) y desde cuándo existe?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"American Data Networks S.A. (ADN) es una empresa de telecomunicaciones fundada en 2005.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¿Cuál es la misión de ADN?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"Ser la primera opción en servicios de telecomunicaciones, garantizando transporte de datos rápido, eficiente y seguro.\"\n",
    "    # },\n",
    "    {\n",
    "        \"question\": \"¿Y la visión de la empresa cuál es? Y desde cuándo existe?\",\n",
    "        \"contexts\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"ground_truth\": \"Ser la compañía líder en vanguardia tecnológica en Costa Rica, con enfoque claro en clientes y estándares de clase mundial. Fue Fundada en 2005\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"question\": \"Estoy armando mi plan en casa: ¿cuánto cuesta el plan residencial de 100/100 megas?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"₡20 500 IVI.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¿Cuál es el precio del plan de 250/250 megas simétricos?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"₡26 600 IVI.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Quiero más velocidad: ¿cuánto vale el plan de 500/500 megas?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"₡33 410 IVI.\"\n",
    "    # },\n",
    "    {\n",
    "        \"question\": \"¿Tienen plan de 1 giga simétrico? ¿En cuánto sale al mes?\",\n",
    "        \"contexts\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"ground_truth\": \"₡49 500 IVI por mes.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"¿Qué incluyen los planes residenciales además del Internet?\",\n",
    "        \"contexts\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"ground_truth\": \"Incluyen equipo Wi-Fi, firewall gestionado y soporte técnico 24/7.\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"question\": \"Quiero agregar telefonía fija a mi plan: ¿cuánto cuesta al mes?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"₡3 590 IVI/mes.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Si compro IPTV, ¿cuántos dispositivos como máximo puedo adquirir?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"Hasta 10 dispositivos IPTV.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¿Y cuánto cuesta cada dispositivo IPTV?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"₡4 590 IVI por dispositivo.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Más o menos, ¿cuántos canales trae el servicio de IPTV?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"Más de 100 canales.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¿Qué impuestos y cargos ya vienen incluidos en los precios IVI?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"IVA 13 %, 1 % Cruz Roja y 0,75 % 9-1-1.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"¿Cuál es el número de WhatsApp para comprar o pedir soporte?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"+506 7087-8240.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"question\": \"Necesito llamarles: ¿cuál es el teléfono principal de ADN?\",\n",
    "    #     \"contexts\": \"\",\n",
    "    #     \"answer\": \"\",\n",
    "    #     \"ground_truth\": \"+506 4050-5050.\"\n",
    "    # }\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Generating test data for ALL retrievers...\")\n",
    "\n",
    "# Initialize storage for all retrievers\n",
    "retriever_responses = {\n",
    "    \"naive\": [],\n",
    "    \"semantic\": [],\n",
    "    \"bm25\": [],\n",
    "    \"multi_query\": [],\n",
    "    \"parent_document\": []\n",
    "}\n",
    "\n",
    "# Dictionary of retrieval chains\n",
    "retrieval_chains = {\n",
    "    \"naive\": naive_retrieval_chain,\n",
    "    \"semantic\": semantic_retrieval_chain,\n",
    "    \"bm25\": bm25_retrieval_chain,\n",
    "    \"multi_query\": multi_query_retrieval_chain,\n",
    "    \"parent_document\": parent_document_retrieval_chain\n",
    "}\n",
    "\n",
    "# Generate responses for each retriever\n",
    "for retriever_name, chain in retrieval_chains.items():\n",
    "    print(f\"\\nGenerating test data for {retriever_name.upper()} retriever...\")\n",
    "    \n",
    "    for i, item in enumerate(manual_test_set_updated):\n",
    "        question = item[\"question\"]\n",
    "        \n",
    "        try:\n",
    "            # Use the specific retrieval chain\n",
    "            response = chain.invoke({\"question\": question})\n",
    "            \n",
    "            updated_item = {\n",
    "                \"question\": question,\n",
    "                \"contexts\": [doc.page_content for doc in response['context']],\n",
    "                \"answer\": response['response'].content,\n",
    "                \"ground_truth\": item[\"ground_truth\"]\n",
    "            }\n",
    "            \n",
    "            retriever_responses[retriever_name].append(updated_item)\n",
    "            print(f\"✓ Processed question {i+1}: {question[:40]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing question {i+1} for {retriever_name}: {str(e)}\")\n",
    "            # Add placeholder data to maintain consistency\n",
    "            updated_item = {\n",
    "                \"question\": question,\n",
    "                \"contexts\": [\"Error: Could not retrieve context\"],\n",
    "                \"answer\": \"Error: Could not generate answer\",\n",
    "                \"ground_truth\": item[\"ground_truth\"]\n",
    "            }\n",
    "            retriever_responses[retriever_name].append(updated_item)\n",
    "\n",
    "print(\"\\n✓ All retriever test data generated!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c38bbe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive dataset shape: (3, 4)\n",
      "Semantic dataset shape: (3, 4)\n",
      "Bm25 dataset shape: (3, 4)\n",
      "Multi_query dataset shape: (3, 4)\n",
      "Parent_document dataset shape: (3, 4)\n",
      "\n",
      "✓ All datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Create datasets for all retrievers\n",
    "retriever_datasets = {}\n",
    "retriever_dataframes = {}\n",
    "\n",
    "for retriever_name, responses in retriever_responses.items():\n",
    "    df = pd.DataFrame(responses)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    retriever_datasets[retriever_name] = dataset\n",
    "    retriever_dataframes[retriever_name] = df\n",
    "    \n",
    "    print(f\"{retriever_name.capitalize()} dataset shape: {df.shape}\")\n",
    "\n",
    "print(\"\\n✓ All datasets created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ef78e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation of all retrievers...\n",
      "============================================================\n",
      "\n",
      "Evaluating NAIVE retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c06c2ee13f4368bf600a9628e1c13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Naive evaluation completed successfully!\n",
      "  faithfulness: 0.9333\n",
      "  answer_relevancy: 0.9188\n",
      "  context_precision: 0.3931\n",
      "  context_recall: 0.6667\n",
      "\n",
      "Evaluating SEMANTIC retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35acd67d27a74eddb947d26548a32e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semantic evaluation completed successfully!\n",
      "  faithfulness: 0.9333\n",
      "  answer_relevancy: 0.9165\n",
      "  context_precision: 0.2264\n",
      "  context_recall: 0.6667\n",
      "\n",
      "Evaluating BM25 retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8dd84beb0f46bb9caf7763ad743c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bm25 evaluation completed successfully!\n",
      "  faithfulness: 1.0000\n",
      "  answer_relevancy: 0.3249\n",
      "  context_precision: 0.0000\n",
      "  context_recall: 0.0000\n",
      "\n",
      "Evaluating MULTI_QUERY retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53288e8f274a4f2ebe5289d96a994d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi_query evaluation completed successfully!\n",
      "  faithfulness: 0.8917\n",
      "  answer_relevancy: 0.9239\n",
      "  context_precision: 0.2819\n",
      "  context_recall: 0.6667\n",
      "\n",
      "Evaluating PARENT_DOCUMENT retriever...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aabfe3e692d4289a63b5998c523bacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parent_document evaluation completed successfully!\n",
      "  faithfulness: 0.9333\n",
      "  answer_relevancy: 0.9231\n",
      "  context_precision: 0.4630\n",
      "  context_recall: 0.6667\n",
      "\n",
      "============================================================\n",
      "✅ ALL EVALUATIONS COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy, \n",
    "    context_precision,\n",
    "    context_recall\n",
    "]\n",
    "\n",
    "# Storage for evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "def extract_score_from_result(result, metric_name):\n",
    "    \"\"\"Safely extract score from RAGAS EvaluationResult object.\"\"\"\n",
    "    try:\n",
    "        # Try dictionary-style access first\n",
    "        score = result[metric_name]\n",
    "        if isinstance(score, list):\n",
    "            # Calculate average for list of scores\n",
    "            valid_scores = [s for s in score if s is not None and not np.isnan(s)]\n",
    "            return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
    "        elif isinstance(score, (int, float)) and not np.isnan(score):\n",
    "            return float(score)\n",
    "        else:\n",
    "            return 0.0\n",
    "    except (KeyError, TypeError, AttributeError):\n",
    "        try:\n",
    "            # Try attribute access as fallback\n",
    "            score = getattr(result, metric_name, None)\n",
    "            if score is not None:\n",
    "                if isinstance(score, list):\n",
    "                    valid_scores = [s for s in score if s is not None and not np.isnan(s)]\n",
    "                    return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
    "                elif isinstance(score, (int, float)) and not np.isnan(score):\n",
    "                    return float(score)\n",
    "            return 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "print(\"Starting comprehensive evaluation of all retrievers...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for retriever_name, dataset in retriever_datasets.items():\n",
    "    print(f\"\\nEvaluating {retriever_name.upper()} retriever...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = evaluate(\n",
    "            dataset,\n",
    "            metrics=metrics,\n",
    "            llm=generator_llm,\n",
    "            embeddings=generator_embeddings\n",
    "        )\n",
    "        \n",
    "        evaluation_results[retriever_name] = result\n",
    "        print(f\"✅ {retriever_name.capitalize()} evaluation completed successfully!\")\n",
    "        \n",
    "        # Show immediate results using corrected extraction\n",
    "        for metric_name in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:\n",
    "            avg_score = extract_score_from_result(result, metric_name)\n",
    "            print(f\"  {metric_name}: {avg_score:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {retriever_name}: {str(e)}\")\n",
    "        # Create placeholder result\n",
    "        evaluation_results[retriever_name] = {\n",
    "            'faithfulness': 0.0,\n",
    "            'answer_relevancy': 0.0, \n",
    "            'context_precision': 0.0,\n",
    "            'context_recall': 0.0\n",
    "        }\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ ALL EVALUATIONS COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33eb1c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "                    COMPREHENSIVE RAG RETRIEVER PERFORMANCE COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "           Metric  Naive  Semantic   Bm25  Multi Query  Parent Document                      Description\n",
      "     Faithfulness 0.9333    0.9333 1.0000       0.8917           0.9333 Factual consistency with context\n",
      " Answer Relevancy 0.9188    0.9165 0.3249       0.9239           0.9231        Relevance to the question\n",
      "Context Precision 0.3931    0.2264 0.0000       0.2819           0.4630   Precision of retrieved context\n",
      "   Context Recall 0.6667    0.6667 0.0000       0.6667           0.6667       Recall of relevant context\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                                          OVERALL AVERAGES                                          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "🥇 Parent Document     : 0.7465\n",
      "🥈 Naive               : 0.7280\n",
      "🥉 Multi Query         : 0.6910\n",
      "4️⃣ Semantic            : 0.6857\n",
      "5️⃣ Bm25                : 0.6624\n",
      "\n",
      "🏆 WINNER: PARENT DOCUMENT RETRIEVER!\n",
      "Best Overall Score: 0.7465\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_score_safe(result, metric_name):\n",
    "    \"\"\"Safely extract score from RAGAS result with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        # Handle both EvaluationResult objects and dictionaries\n",
    "        if hasattr(result, '__getitem__'):\n",
    "            try:\n",
    "                score = result[metric_name]\n",
    "            except (KeyError, TypeError):\n",
    "                # Fallback to attribute access\n",
    "                score = getattr(result, metric_name, None)\n",
    "        else:\n",
    "            # Try attribute access first\n",
    "            score = getattr(result, metric_name, None)\n",
    "        \n",
    "        if score is None:\n",
    "            return 0.0\n",
    "            \n",
    "        if isinstance(score, list):\n",
    "            # Calculate average for list of scores\n",
    "            valid_scores = [s for s in score if s is not None and not (isinstance(s, float) and np.isnan(s))]\n",
    "            return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
    "        elif isinstance(score, (int, float)) and not np.isnan(score):\n",
    "            return float(score)\n",
    "        else:\n",
    "            return 0.0\n",
    "    except (KeyError, TypeError, AttributeError) as e:\n",
    "        print(f\"Warning: Could not extract {metric_name}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# Extract scores for all retrievers\n",
    "metrics_names = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "retriever_names = ['naive', 'semantic', 'bm25', 'multi_query', 'parent_document']\n",
    "\n",
    "# Create comprehensive results matrix\n",
    "results_matrix = {}\n",
    "for retriever_name in retriever_names:\n",
    "    result = evaluation_results[retriever_name]\n",
    "    scores = [extract_score_safe(result, metric) for metric in metrics_names]\n",
    "    results_matrix[retriever_name] = scores\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Faithfulness',\n",
    "        'Answer Relevancy',\n",
    "        'Context Precision', \n",
    "        'Context Recall'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add columns for each retriever\n",
    "for retriever_name in retriever_names:\n",
    "    comparison_data[retriever_name.replace('_', ' ').title()] = results_matrix[retriever_name]\n",
    "\n",
    "# Add descriptions\n",
    "comparison_data['Description'] = [\n",
    "    'Factual consistency with context',\n",
    "    'Relevance to the question',\n",
    "    'Precision of retrieved context',\n",
    "    'Recall of relevant context'\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Calculate averages for each retriever\n",
    "averages = {}\n",
    "for retriever_name in retriever_names:\n",
    "    scores = results_matrix[retriever_name]\n",
    "    valid_scores = [score for score in scores if score > 0]\n",
    "    averages[retriever_name] = np.mean(valid_scores) if valid_scores else 0.0\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"                    COMPREHENSIVE RAG RETRIEVER PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "# Display main comparison table\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "print()\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'OVERALL AVERAGES':^100}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Sort retrievers by performance\n",
    "sorted_retrievers = sorted(averages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (retriever_name, avg_score) in enumerate(sorted_retrievers):\n",
    "    rank_emoji = \"🥇\" if i == 0 else \"🥈\" if i == 1 else \"🥉\" if i == 2 else f\"{i+1}️⃣\"\n",
    "    print(f\"{rank_emoji} {retriever_name.replace('_', ' ').title():<20}: {avg_score:.4f}\")\n",
    "\n",
    "print()\n",
    "best_retriever = sorted_retrievers[0][0]\n",
    "best_score = sorted_retrievers[0][1]\n",
    "print(f\"🏆 WINNER: {best_retriever.replace('_', ' ').title().upper()} RETRIEVER!\")\n",
    "print(f\"Best Overall Score: {best_score:.4f}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77847653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                        DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "METRIC-BY-METRIC WINNERS:\n",
      "----------------------------------------\n",
      "📊 Faithfulness:\n",
      "   🏆 Best:  Bm25 (1.0000)\n",
      "   📉 Worst: Multi Query (0.8917)\n",
      "\n",
      "📊 Answer Relevancy:\n",
      "   🏆 Best:  Multi Query (0.9239)\n",
      "   📉 Worst: Bm25 (0.3249)\n",
      "\n",
      "📊 Context Precision:\n",
      "   🏆 Best:  Parent Document (0.4630)\n",
      "   📉 Worst: Bm25 (0.0000)\n",
      "\n",
      "📊 Context Recall:\n",
      "   🏆 Best:  Naive (0.6667)\n",
      "   📉 Worst: Bm25 (0.0000)\n",
      "\n",
      "RETRIEVER STRENGTHS & WEAKNESSES:\n",
      "---------------------------------------------\n",
      "🔍 Naive:\n",
      "   ✅ Strongest: Faithfulness (0.9333)\n",
      "   ⚠️  Weakest:  Context Precision (0.3931)\n",
      "\n",
      "🔍 Semantic:\n",
      "   ✅ Strongest: Faithfulness (0.9333)\n",
      "   ⚠️  Weakest:  Context Precision (0.2264)\n",
      "\n",
      "🔍 Bm25:\n",
      "   ✅ Strongest: Faithfulness (1.0000)\n",
      "   ⚠️  Weakest:  Context Precision (0.0000)\n",
      "\n",
      "🔍 Multi Query:\n",
      "   ✅ Strongest: Answer Relevancy (0.9239)\n",
      "   ⚠️  Weakest:  Context Precision (0.2819)\n",
      "\n",
      "🔍 Parent Document:\n",
      "   ✅ Strongest: Faithfulness (0.9333)\n",
      "   ⚠️  Weakest:  Context Precision (0.4630)\n",
      "\n",
      "🎯 PRODUCTION RECOMMENDATIONS:\n",
      "-----------------------------------\n",
      "✅ Deploy PARENT DOCUMENT RETRIEVER for production\n",
      "✅ Excellent for maintaining document context integrity\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                        DETAILED PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Metric-by-metric analysis\n",
    "print(\"\\nMETRIC-BY-METRIC WINNERS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, metric in enumerate(comparison_df['Metric']):\n",
    "    metric_scores = {}\n",
    "    for j, retriever_name in enumerate(retriever_names):\n",
    "        col_name = retriever_name.replace('_', ' ').title()\n",
    "        metric_scores[retriever_name] = comparison_df.iloc[i][col_name]\n",
    "    \n",
    "    # Find best performer for this metric\n",
    "    best_performer = max(metric_scores.items(), key=lambda x: x[1])\n",
    "    worst_performer = min(metric_scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"📊 {metric}:\")\n",
    "    print(f\"   🏆 Best:  {best_performer[0].replace('_', ' ').title()} ({best_performer[1]:.4f})\")\n",
    "    print(f\"   📉 Worst: {worst_performer[0].replace('_', ' ').title()} ({worst_performer[1]:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Strengths and weaknesses analysis\n",
    "print(\"RETRIEVER STRENGTHS & WEAKNESSES:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for retriever_name in retriever_names:\n",
    "    scores = results_matrix[retriever_name]\n",
    "    best_metric_idx = np.argmax(scores)\n",
    "    worst_metric_idx = np.argmin(scores)\n",
    "    \n",
    "    best_metric = metrics_names[best_metric_idx]\n",
    "    worst_metric = metrics_names[worst_metric_idx]\n",
    "    \n",
    "    print(f\"🔍 {retriever_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"   ✅ Strongest: {best_metric.replace('_', ' ').title()} ({scores[best_metric_idx]:.4f})\")\n",
    "    print(f\"   ⚠️  Weakest:  {worst_metric.replace('_', ' ').title()} ({scores[worst_metric_idx]:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Recommendations\n",
    "print(\"🎯 PRODUCTION RECOMMENDATIONS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "if best_retriever == \"semantic\":\n",
    "    print(\"✅ Deploy SEMANTIC RETRIEVER for production\")\n",
    "    print(\"✅ Excellent balance of context precision and recall\")\n",
    "elif best_retriever == \"multi_query\":\n",
    "    print(\"✅ Deploy MULTI-QUERY RETRIEVER for production\") \n",
    "    print(\"✅ Superior query understanding and context retrieval\")\n",
    "elif best_retriever == \"parent_document\":\n",
    "    print(\"✅ Deploy PARENT DOCUMENT RETRIEVER for production\")\n",
    "    print(\"✅ Excellent for maintaining document context integrity\")\n",
    "elif best_retriever == \"bm25\":\n",
    "    print(\"✅ Deploy BM25 RETRIEVER for production\")\n",
    "    print(\"✅ Strong traditional IR performance with keyword matching\")\n",
    "else:\n",
    "    print(\"✅ Deploy NAIVE RETRIEVER for production\")\n",
    "    print(\"✅ Simple and effective baseline performance\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
